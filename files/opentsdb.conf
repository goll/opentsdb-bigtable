# --------- NETWORK ----------
# The TCP port TSD should use for communications
# *** REQUIRED ***
tsd.network.port = 4242

# The IPv4 network address to bind to, defaults to all addresses
# tsd.network.bind = 0.0.0.0

# Disable Nagel's algorithm, default is True
#tsd.network.tcp_no_delay = true

# Determines whether or not to send keepalive packets to peers, default is True
#tsd.network.keep_alive = true

# Determines if the same socket should be used for new connections, default is True
#tsd.network.reuse_address = true

# Number of worker threads dedicated to Netty, defaults to # of CPUs * 2
#tsd.network.worker_threads = 8

# Whether or not to use NIO or tradditional blocking IO, defaults to True
#tsd.network.async_io = true

# The connection queue depth for completed or incomplete connection requests depending on OS.
# The default may be limited by the 'somaxconn' kernel setting or set by Netty to 3072.
tsd.network.backlog = 128

# ----------- HTTP -----------
# The location of static files for the HTTP GUI interface.
# *** REQUIRED ***
tsd.http.staticroot = /opt/opentsdb/staticroot

# Where TSD should write it's cache files to
# *** REQUIRED ***
tsd.http.cachedir = /tmp/opentsdb

# Whether or not to enable incoming chunk support for the HTTP RPC, default is false
tsd.http.request.enable_chunked = true

# The maximum request body size to support for incoming HTTP requests when chunking is enabled, default is 4096
tsd.http.request.max_chunk = 1048576

# --------- CORE ----------
# Whether or not to automatically create UIDs for new metric types, default is False
#tsd.core.auto_create_metrics = false

# Whether or not to enable the built-in UI Rpc Plugins, default is True
#tsd.core.enable_ui = true

# Whether or not to enable the built-in API Rpc Plugins, default is True
#tsd.core.enable_api = true

# --------- STORAGE ----------
# Whether or not to enable data compaction, default is True
tsd.storage.enable_compaction = False

# How often, in milliseconds, to flush the data point queue to storage, default is 1,000
# tsd.storage.flush_interval = 1000

# Max number of rows to be returned per Scanner round trip
# tsd.storage.hbase.scanner.maxNumRows = 128

# Name of the HBase table where data points are stored, default is "tsdb"
#tsd.storage.hbase.data_table = tsdb

# Name of the HBase table where UID information is stored, default is "tsdb-uid"
#tsd.storage.hbase.uid_table = tsdb-uid

# Name of the HBase table where meta data are stored, default is "tsdb-meta"
#tsd.storage.hbase.meta_table = tsdb-meta

# Name of the HBase table where tree data are stored, default is "tsdb-tree"
#tsd.storage.hbase.tree_table = tsdb-tree

# Path under which the znode for the -ROOT- region is located, default is "/hbase"
#tsd.storage.hbase.zk_basedir = /hbase

# A comma separated list of Zookeeper hosts to connect to, with or without
# port specifiers, default is "localhost"
#tsd.storage.hbase.zk_quorum = localhost

# Whether or not to accept the last written value when parsing data points with duplicate timestamps.
# When enabled in conjunction with compactions, a compacted column will be written with the latest data points.
# Default is False
#tsd.storage.fix_duplicates = False

# The maximum number of tags allowed per data point, default is 8
#tsd.storage.max_tags = 8

# The width, in bytes, of metric UIDs, default is 3
# WARNING Do not change after writing data or you will corrupt your tables and not be able to query any more.
#tsd.storage.uid.width.metric = 3

# The width, in bytes, of tag name UIDs, default is 3
# WARNING Do not change after writing data or you will corrupt your tables and not be able to query any more.
#tsd.storage.uid.width.tagk = 3

# The width, in bytes, of tag value UIDs, default is 3
# WARNING Do not change after writing data or you will corrupt your tables and not be able to query any more.
#tsd.storage.uid.width.tagv = 3

# --------- COMPACTIONS ---------------------------------
# Frequency at which compaction thread wakes up to flush stuff in seconds, default 10
# tsd.storage.compaction.flush_interval = 10

# Minimum rows attempted to compact at once, default 100
# tsd.storage.compaction.min_flush_threshold = 100

# Maximum number of rows, compacted concirrently, default 10000
# tsd.storage.compaction.max_concurrent_flushes = 10000

# Compaction flush speed multiplier, default 2
# tsd.storage.compaction.flush_speed = 2

# --------- BIGTABLE ----------
# The project ID hosting your Bigtable cluster
google.bigtable.project.id = PROJECT-ID

# Bigtable instance ID
google.bigtable.instance.id = INSTANCE-ID

# The class that will be used to implement the HBase API AsyncBigtable will use as a shim between the Bigtable client and OpenTSDB
# Set this to com.google.cloud.bigtable.hbase1_0.BigtableConnection
hbase.client.connection.impl = com.google.cloud.bigtable.hbase1_2.BigtableConnection

# Whether or not to use a Google cloud service account to connect
# Set this to true
google.bigtable.auth.service.account.enable = true

# The full path to the JSON formatted key file associated with the service account you want to use for Bigtable access.
# Download this from your cloud console.
google.bigtable.auth.json.keyfile = /opt/opentsdb/bigtable.json

# The number of sockets opened to the Bigtable API for handling RPCs.
# For higher throughput consider increasing the channel count, default is 4
#google.bigtable.grpc.channel.count = 4
